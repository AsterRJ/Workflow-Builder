{
  "name": "Llama Builder - Combined (Main + Sub-workflow)",
  "nodes": [
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "value": "vlJlmNoBNXlsGGvp",
          "mode": "list",
          "cachedResultUrl": "/workflow/vlJlmNoBNXlsGGvp",
          "cachedResultName": "Llama Builder - Combined (Main + Sub-workflow)"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {},
          "matchingColumns": [],
          "schema": [
            {
              "id": "job_id",
              "displayName": "job_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string"
            },
            {
              "id": "job_label",
              "displayName": "job_label",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string"
            },
            {
              "id": "job_description",
              "displayName": "job_description",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string"
            },
            {
              "id": "preconditions_text",
              "displayName": "preconditions_text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string"
            },
            {
              "id": "ouctomes",
              "displayName": "ouctomes",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "array"
            },
            {
              "id": "skills",
              "displayName": "skills",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "array"
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": true
        },
        "options": {}
      },
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [
        -1648,
        -2144
      ],
      "id": "192c6eb4-5f40-457d-8c86-71863995205e",
      "name": "Execute Subtasks Sub-workflow"
    },
    {
      "parameters": {
        "workflowInputs": {
          "values": [
            {
              "name": "job_id"
            },
            {
              "name": "job_label"
            },
            {
              "name": "job_description"
            },
            {
              "name": "preconditions_text"
            },
            {
              "name": "ouctomes",
              "type": "array"
            },
            {
              "name": "skills",
              "type": "array"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        -3312,
        -1616
      ],
      "id": "9732a4cf-fddc-435f-9c15-143174e740f8",
      "name": "SUB-WORKFLOW: Trigger"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  model: \"llama3\",\n  stream: false,\n  format: \"json\",\n  temperature: 0,\n  messages: [\n    {\n  role: \"system\",\n  content:\n    \"Decompose ONE workflow step into 1 to 3 smaller ordered sub-tasks.\\n\" +\n    \"Each sub-task should be a single action that could be assigned to either a system or a human operator.\\n\" +\n    \"\\n\" +\n    \"You must output ONLY a JSON object with this shape:\\n\" +\n    \"{\\n\" +\n    \"  \\\"subtasks\\\": [\\n\" +\n    \"    { \\n\" +\n    \"      \\\"description\\\": string,\\n\" +\n    \"      \\\"kind\\\": \\\"system\\\" | \\\"human\\\",\\n\" +\n    \"      \\\"is_decision_point\\\": boolean,  // NEW: true if this subtask requires conditional logic\\n\" +\n    \"      \\\"decision_criteria\\\": string,    // NEW: describe what determines pass/fail\\n\" +\n    \"      \\\"pass_condition\\\": string,       // NEW: what constitutes success\\n\" +\n    \"      \\\"fail_action\\\": string           // NEW: what to do on failure (e.g., \\\"notify_manager\\\")\\n\" +\n    \"    }\\n\" +\n    \"  ]\\n\" +\n    \"}\\n\" +\n    \"\\n\" +\n    \"Rules:\\n\" +\n    \"- Use kind = \\\"system\\\" if the action can reasonably be automated.\\n\" +\n    \"- Use kind = \\\"human\\\" if it clearly involves human judgement.\\n\" +\n    \"- Set is_decision_point = true if the subtask's outcome determines whether to continue or abort.\\n\" +\n    \"- For decision points, specify:\\n\" +\n    \"  * decision_criteria: What is being checked (e.g., 'file validation', 'permission check')\\n\" +\n    \"  * pass_condition: When to continue (e.g., 'all files valid', 'user has permission')\\n\" +\n    \"  * fail_action: What to do on failure (typically 'notify_manager' or 'report_to_server')\\n\" +\n    \"- Sub-tasks must be in execution order.\\n\" +\n    \"- No markdown, no explanations, JSON only.\"\n},\n    {\n      role: \"user\",\n      content: JSON.stringify({\n        id: $('SUB-WORKFLOW: Trigger').item.json.job_id,\n        label: $('SUB-WORKFLOW: Trigger').item.json.job_label,\n        description: $('SUB-WORKFLOW: Trigger').item.json.job_description,\n        preconditions: $('SUB-WORKFLOW: Trigger').item.json.preconditions_text || [],\n        possible_outcomes: ($('SUB-WORKFLOW: Trigger').item.json.ouctomes || []).map(o => o.name)\n      })\n    }\n  ]\n}) }}\n",
        "options": {
          "response": {
            "response": {}
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -3088,
        -1616
      ],
      "id": "a1d58b86-a0d8-4566-af14-a585a855df53",
      "name": "SUB: Subskills Splitter"
    },
    {
      "parameters": {
        "jsCode": "const content = $input.first().json.message.content || \"\";\nconst start = content.indexOf(\"{\");\nconst end = content.lastIndexOf(\"}\");\nif (start === -1 || end === -1 || end <= start) {\n  throw new Error(\"Could not locate JSON object inside LLM output.\");\n}\nconst jsonText = content.slice(start, end + 1).trim();\nconsole.log(jsonText)\nlet parsed;\ntry {\n  parsed = JSON.parse(jsonText);\n} catch (e) {\n  throw new Error(\n    \"JSON parsing failed: \" + e.message +\n    \"\\nExtracted snippet:\\n\" + jsonText.slice(0, 300)\n  );\n}\nreturn [{\n  json: parsed\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2880,
        -1616
      ],
      "id": "5812e400-b4a6-4476-bb5d-693e4c27e715",
      "name": "SUB: JSON Split Subtasks"
    },
    {
      "parameters": {
        "jsCode": "return $('SUB: JSON Split Subtasks').first().json.subtasks.map((s, i) => ({\n  json: {\n    step: s,\n    step_index: i}\n}));\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2656,
        -1616
      ],
      "id": "40515b2a-b395-472d-9c4a-d80b1e667915",
      "name": "SUB: Explode Subtasks"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -2432,
        -1616
      ],
      "id": "34616671-92e3-47fc-bbed-4f8de5b73fbb",
      "name": "SUB: Loop Over Subtasks"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  model: \"llama3\",\n  stream: false,\n  temperature: 0,\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are a classifier. You must choose ONE skill_id from a fixed list.\\n\" +\n        \"The user will give you:\\n\" +\n        \"1) A list of skills with id, description, and category.\\n\" +\n        \"2) One workflow step (label, description).\\n\" +\n        \"\\n\" +\n        \"Your job: pick the SINGLE skill_id that best matches what the step is doing.\\n\" +\n        \"Prefer skills whose category and description align with the step.\\n\" +\n        \"\\n\" +\n        \"Output rules:\\n\" +\n        \"- Output ONLY the chosen skill_id string.\\n\" +\n        \"- Do NOT explain. Do NOT add any other text.\\n\" +\n        \"- If absolutely no skill matches, output EXACTLY: NONE\"\n    },\n    {\n      role: \"user\",\n      content:\n        \"Skills:\\n\" +\n        $('SUB-WORKFLOW: Trigger').item.json.skills\n          .map(s => `${s.id} [${s.category}]: ${s.description}`)\n          .join(\"\\n\") +\n        \"\\n\\n\" +\n        \"Step:\\n\" +\n        \"id: \" + $('SUB: Loop Over Subtasks').item.json.step_index + \"\\n\" +\n        \"label: \" + $('SUB: Loop Over Subtasks').item.json.step.description + \"\\n\" +\n        \"description: \" + $('SUB: Loop Over Subtasks').item.json.step.description + \"\\n\\n\" +\n        \"Choose ONE skill_id from the list above.\"\n    }\n  ]\n}) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2064,
        -1600
      ],
      "id": "5e2ae372-76e8-4b97-8a87-c31625582a23",
      "name": "SUB: Extract Tasks"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8011/subtask-update",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  substep_index: $('SUB: Loop Over Subtasks').item.json.step_index,\n  description: $('SUB: Loop Over Subtasks').item.json.step.description,\n  bladerunner_index: $('SUB: Loop Over Subtasks').item.json.step.kind,\n  skill: $json.message.content,\n  decision_node : $('Decision Point Code').item.json.is_decision_point\n}) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1824,
        -1600
      ],
      "id": "5f7118ef-77ef-499b-b270-233cbd679d06",
      "name": "SUB: Subtask Feedback"
    },
    {
      "parameters": {
        "content": "## SUB-WORKFLOW: Process Subtasks\n\nThis section handles the inner loop that was causing nesting issues.\nDoes essentially what the above does, but links it to specific tools your <implementation method>, be it n8n or other, is loaded to handle.\n",
        "height": 424,
        "width": 1768,
        "color": 5
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -3392,
        -1808
      ],
      "id": "3e9fc1e4-7e41-4dde-818e-9b602bb60939",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "content": "## MAIN WORKFLOW\n\nNote: The \"Execute Subtasks Sub-workflow\" node calls the sub-workflow below. \nNecessary due to iffy behaviour when nesting loops. All data feeds back to Python through http calls.\n\nDissects:\n1. The ontology, \n2. The rules of the ontology,\n3. Tasks within the workflow, related to the ontology,\n4. These are passed on to another workflow (below) that handles the problems have when facing computers for the first time: thinking many tasks is just one ('Get to the store. Should be an easy instruction for a car.' - Elon Musk)\n",
        "height": 1108,
        "width": 3644,
        "color": 4
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -3392,
        -2944
      ],
      "id": "7f50d01c-c9d7-48d5-a67c-2461fe8479a1",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "wf_builder_agent",
        "options": {
          "responseData": "={ \"accepted\": true }"
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -3344,
        -2624
      ],
      "id": "3c9e4131-a224-4ea6-8afb-58f0284df84f",
      "name": "Webhook1",
      "webhookId": "9bc4da2b-061a-4f89-81f3-2953e4a9cdce"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  model: \"llama3\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are an ontology normalisation assistant for industrial workflows.\\n\" +\n        \"You will receive:\\n\" +\n        \"1) A candidate ontology skeleton in JSON text (objects, processes, properties, events).\\n\" +\n        \"2) Up to four source documents, clearly separated:\\n\" +\n        \"   [WORKFLOW_SPEC]  - narrative workflow description\\n\" +\n        \"   [WPS]            - formal Procedure Specification (if provided)\\n\" +\n        \"   [DATASHEET]      - resource datasheet(s) (if provided)\\n\" +\n        \"[ONTOLOGY_SPEC]  - pre-existing ontology (if provided)\\n\\n\" + \n        \"Your job is to:\\n\" +\n        \"- Merge synonyms and duplicates across all documents;\\n\" +\n        \"-Identify possible pre-existing ontological elements for objects and processes;\\n\" + \n        \"- Where this is not possible, create stable canonical IDs for objects and processes;\\n\" +\n        \"- Add basic relations using simple types such as:\\n\" +\n        \"    uses, acts_on, part_of, located_in, precondition_of, produces;\\n\" +\n        \"- Maintain the four top-level categories: objects, processes, properties, events.\\n\\n\" +\n        \"Treat WORKFLOW_SPEC as the primary source of intent.\\n\" +\n        \"Use WPS as authoritative for required steps and safety constraints.\\n\" +\n        \"Use DATASHEET only to enrich object properties and constraints.\\n\\n\" +\n        \"Output ONLY valid JSON with keys: objects, processes, properties, events, relations.\\n\" +\n        \"Do NOT include any explanatory text or additional keys.\"\n    },\n    {\n      role: \"user\",\n      content:\n        \"Here is the extracted ontology skeleton (JSON text):\\n\\n\" +\n        $json.message.content + \"\\n\\n\" +\n\n        \"Now here are the source documents:\\n\\n\" +\n        \"[WORKFLOW_SPEC]\\n\" +\n        $node[\"Webhook1\"].json.body.spec_text + \"\\n\\n\" +\n\n        \"[WPS]\\n\" +\n        ($node[\"Webhook1\"].json.body.wps_text || \"NOT PROVIDED\") + \"\\n\\n\" +\n\n        \"[DATASHEET]\\n\" +\n        ($node[\"Webhook1\"].json.body.datasheet_text || \"NOT PROVIDED\") + \"\\n\\n\" +\n        \"[ONTOLOGY_SPEC]\\n\" +\n        JSON.stringify($node[\"Extract Objects1\"].json) \n    }\n  ],\n  stream: false\n}) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2272,
        -2544
      ],
      "id": "0ce5d969-d9d8-4247-8421-05fb1449cc72",
      "name": "Ontology Normalisation1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  model: \"llama3\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are a rule extraction engine for an industrial ontology.\\n\" +\n        \"You will receive:\\n\" +\n        \"1) A base ontology in JSON text (objects, processes, properties, relations).\\n\" +\n        \"2) Up to three source documents, clearly separated as:\\n\" +\n        \"   [WORKFLOW_SPEC]  - narrative workflow description\\n\" +\n        \"   [WPS]            - formal Welding Procedure Specification (if provided)\\n\" +\n        \"   [DATASHEET]      - tool/material datasheet(s) (if provided)\\n\\n\" +\n        \"Your job is to extract a set of formal rules of the following types:\\n\" +\n        \"  - safety\\n\" +\n        \"  - operational_limit\\n\" +\n        \"  - precondition\\n\" +\n        \"  - postcondition\\n\" +\n        \"  - invariant\\n\\n\" +\n        \"DEFINITIONS\\n\" +\n        \"- precondition: must be true BEFORE a process or step starts.\\n\" +\n        \"- postcondition: becomes true AFTER a process or step completes.\\n\" +\n        \"- safety: any rule whose main purpose is to protect people, equipment, or environment.\\n\" +\n        \"- operational_limit: numeric or symbolic operating bounds (for example ranges, maxima, minima, rated capacities).\\n\" +\n        \"- invariant: a condition that must remain true THROUGHOUT a process or across multiple steps.\\n\" +\n        \"  * Look for wording such as: \\\"at all times\\\", \\\"throughout\\\", \\\"must always\\\", \\\"shall remain\\\", \\\"must not at any time\\\", \\\"never\\\".\\n\" +\n        \"  * Typical examples: restrictions on properties or behaviours, forbidden actions, conditions that must be maintained.\\n\\n\" +\n        \"SOURCES\\n\" +\n        \"- Treat [WPS] as the authoritative source for required steps, required sequences and safety constraints.\\n\" +\n        \"- Treat [WORKFLOW_SPEC] as contextual, describing how operators actually perform or describe the steps.\\n\" +\n        \"- Treat [DATASHEET] as the main source of numeric/technical limits and invariants over parameters (for example current, voltage, pressure, speed, temperature).\\n\" +\n        \"- When possible, turn numeric ranges or absolute limits from WPS/DATASHEET into operational_limit or invariant rules.\\n\\n\" +\n        \"MAPPING TO ONTOLOGY\\n\" +\n        \"- Rules MUST reference ontology IDs exactly as they appear in the ontology JSON.\\n\" +\n        \"- When defining an invariant or safety rule, link it to the relevant ontology process and/or objects.\\n\" +\n        \"- If the document text uses free-form wording, map it to the closest ontology process/object/property and use that ID.\\n\\n\" +\n        \"OUTPUT FORMAT\\n\" +\n        \"Output ONLY valid JSON in the form:\\n\" +\n        \"{ \\\"rules\\\": [ ... ] }\\n\" +\n        \"Each element in \\\"rules\\\" must be a JSON object with at least:\\n\" +\n        \"  - \\\"rule_type\\\": one of [\\\"safety\\\", \\\"operational_limit\\\", \\\"precondition\\\", \\\"postcondition\\\", \\\"invariant\\\"],\\n\" +\n        \"  - \\\"description\\\": human-readable text of the rule,\\n\" +\n        \"  - \\\"applies_to\\\": an array of ontology IDs (objects/processes/properties/relations) that this rule constrains.\\n\" +\n        \"You MAY add other fields (for example \\\"source_document\\\", \\\"source_span\\\", \\\"parameters\\\") if helpful, but do NOT add any other top-level keys besides \\\"rules\\\".\\n\" +\n        \"Do NOT include any free text outside the JSON.\"\n    },\n    {\n      role: \"user\",\n      content:\n        \"Ontology (JSON text):\\n\\n\" +\n        $node[\"Ontology Normalisation1\"].json.message.content +\n        \"\\n\\n\" +\n        \"Source documents:\\n\\n\" +\n        \"[WORKFLOW_SPEC]\\n\" +\n        $node[\"Webhook1\"].json.body.spec_text + \"\\n\\n\" +\n        \"[WPS]\\n\" +\n        ($node[\"Webhook1\"].json.body.wps_text || \"NOT PROVIDED\") + \"\\n\\n\" +\n        \"[DATASHEET]\\n\" +\n        ($node[\"Webhook1\"].json.body.datasheet_text || \"NOT PROVIDED\")\n    }\n  ],\n  stream: false\n}) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1552,
        -2528
      ],
      "id": "664c59c8-415c-4e74-9c37-f712e8827ef2",
      "name": "Extract Rules1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify(\n\n{\n  model: \"llama3\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are a workflow text parser.\\n\" +\n        \"\\n\" +\n        \"You receive:\\n\" +\n        \"- Original workflow specification text written for humans (numbered steps, bullets, paragraphs or mixed prose).\\n\" +\n        \"- Optionally, a CURRENT WORKFLOW JSON (steps + transitions) representing the last parsed version.\\n\" +\n        \"- Optionally, REFINEMENT INSTRUCTIONS describing corrections or changes requested by a critic or by the user.\\n\" +\n        \"\\n\" +\n        \"Your job is to output a STRUCTURED WORKFLOW. You must NOT use any ontology identifiers.\\n\" +\n        \"\\n\" +\n        \"BEHAVIOUR WITH CURRENT WORKFLOW AND REFINEMENTS\\n\" +\n        \"- If CURRENT WORKFLOW is NOT PROVIDED or clearly empty, parse the specification text from scratch as a new workflow.\\n\" +\n        \"- If CURRENT WORKFLOW IS PROVIDED:\\n\" +\n        \"  * Treat it as your starting point.\\n\" +\n        \"  * Apply the REFINEMENT INSTRUCTIONS and any new information in the specification text.\\n\" +\n        \"  * Preserve step IDs where the step meaning is unchanged.\\n\" +\n        \"  * Update labels, descriptions and connections only where required by the refinements.\\n\" +\n        \"  * If refinements ask for new steps, add new step IDs (e.g. stepX_new) without reusing existing IDs.\\n\" +\n        \"  * If refinements ask to remove steps, remove them and any transitions that reference them.\\n\" +\n        \"- If there is a direct conflict between the old workflow and the specification text, the specification text takes precedence.\\n\" +\n        \"- If there is a direct conflict between the old workflow and the REFINEMENT INSTRUCTIONS, the REFINEMENT INSTRUCTIONS take precedence.\\n\" +\n        \"\\n\" +\n        \"MAIN TASK\\n\" +\n        \"You must ensure the FINAL RESULT is a single coherent workflow, even when refinements were applied.\\n\" +\n        \"You must:\\n\" +\n        \"1) Identify individual workflow steps in the order they occur.\\n\" +\n        \"2) For each step, ensure you have:\\n\" +\n        \"   - A short human-readable label (label).\\n\" +\n        \"   - A detailed description in your own words.\\n\" +\n        \"   - Any preconditions, postconditions, and invariants that are implied by the text or by refinements.\\n\" +\n        \"3) Identify decision points and outcomes:\\n\" +\n        \"   - If the text or refinements describe alternative paths, identify these as possible_outcomes for the relevant step.\\n\" +\n        \"4) Build a step-level graph via transitions between steps, including conditional branches.\\n\" +\n        \"\\n\" +\n        \"SCHEMA\\n\" +\n        \"Return ONLY a JSON object with this structure:\\n\" +\n        \"{\\n\" +\n        \"  \\\"steps\\\": [\\n\" +\n        \"    {\\n\" +\n        \"      \\\"id\\\": string,                      // e.g. \\\"step1\\\", \\\"step2\\\"\\n\" +\n        \"      \\\"label\\\": string,                   // short name, e.g. \\\"Visual inspection\\\"\\n\" +\n        \"      \\\"description\\\": string,             // full description\\n\" +\n        \"      \\\"preconditions_text\\\": string[],    // textual, may be empty\\n\" +\n        \"      \\\"postconditions_text\\\": string[],   // textual, may be empty\\n\" +\n        \"      \\\"invariants_text\\\": string[],       // textual, may be empty\\n\" +\n        \"      \\\"possible_outcomes\\\": [             // may be empty if no decisions\\n\" +\n        \"        {\\n\" +\n        \"          \\\"name\\\": string,                // machine-friendly, e.g. \\\"defects_found\\\"\\n\" +\n        \"          \\\"description\\\": string,         // human description\\n\" +\n        \"          \\\"is_terminal\\\": boolean         // true if this outcome ends the workflow\\n\" +\n        \"        }\\n\" +\n        \"      ]\\n\" +\n        \"    }\\n\" +\n        \"  ],\\n\" +\n        \"  \\\"transitions\\\": [\\n\" +\n        \"    {\\n\" +\n        \"      \\\"source_step_id\\\": string,          // must match a step id\\n\" +\n        \"      \\\"target_step_id\\\": string,          // must match a step id\\n\" +\n        \"      \\\"condition_text\\\": string | null,   // textual condition; omit or null if unconditional\\n\" +\n        \"      \\\"outcome_name\\\": string | null      // name of a possible_outcome, or omit/null\\n\" +\n        \"    }\\n\" +\n        \"  ]\\n\" +\n        \"}\\n\" +\n        \"\\n\" +\n        \"RULES\\n\" +\n        \"- Treat words like \\\"if\\\", \\\"when\\\", \\\"unless\\\", \\\"otherwise\\\", \\\"only if\\\", \\\"in that case\\\", \\\"then\\\", \\\"else\\\" as decision cues.\\n\" +\n        \"- If there are clearly different paths, you MUST create possible_outcomes and transitions with condition_text.\\n\" +\n        \"- Do NOT invent domain-specific semantics; stay close to the original wording and to the refinements.\\n\" +\n        \"- If the text is purely linear with no decisions, you may produce transitions with null/omitted condition_text and empty possible_outcomes.\\n\" +\n        \"- The final JSON must be syntactically valid and contain no comments or extra text.\\n\" +\n        \"- Always output the FULL UPDATED workflow, not just a diff.\"\n    },\n    {\n      role: \"user\",\n      content:\n        \"Original workflow specification text:\\n\\n\" +\n        $node[\"Webhook1\"].json.body.spec_text +\n        \"\\n\\nCurrent workflow JSON (may be 'NOT PROVIDED' or empty):\\n\\n\" +\n        ($json.current_workflow || \"NOT PROVIDED\") +\n        \"\\n\\nRefinement instructions (may be 'NONE' or empty):\\n\\n\" +\n        ($json.refinement_instructions || \"NONE\")\n    }\n  ],\n  stream: false\n}\n\n) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -656,
        -2544
      ],
      "id": "cd1b9ea2-871f-42d4-907a-6dc750787b05",
      "name": "Task Partition1"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "3ec8b2dc-eff6-4b03-b9b0-9f5af5fe1daa",
              "name": "iteration_number",
              "value": 0,
              "type": "number"
            },
            {
              "id": "9f9a3f48-1b90-464b-a3af-4660d6d8d697",
              "name": "refinement_instructions",
              "value": "\"\"",
              "type": "string"
            },
            {
              "id": "4489575d-7a71-4326-a1bb-31eeff65ea31",
              "name": "current_ontology",
              "value": "\"\"",
              "type": "string"
            },
            {
              "id": "10c6a244-b276-46b4-ab0d-aa2a30171ffe",
              "name": "current_workflow",
              "value": "\"\"",
              "type": "string"
            },
            {
              "id": "3a05e370-5339-427d-a074-0a8617ff1261",
              "name": "clean_ontology",
              "value": "\"\"",
              "type": "string"
            },
            {
              "id": "bd1d90a5-977b-4614-9f2f-451468bee6c6",
              "name": "clean_rules",
              "value": "\"\"",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -3152,
        -2624
      ],
      "id": "3a0cba05-f86e-4840-9f16-4786bb7f8473",
      "name": "Setup1"
    },
    {
      "parameters": {
        "jsCode": "const content = $input.first().json.message.content || \"\";\nconst start = content.indexOf(\"{\");\nconst end = content.lastIndexOf(\"}\");\nif (start === -1 || end === -1 || end <= start) {\n  throw new Error(\"Could not locate JSON object inside LLM output.\");\n}\nconst jsonText = content.slice(start, end + 1).trim();\nconsole.log(jsonText)\nlet parsed;\ntry {\n  parsed = JSON.parse(jsonText);\n} catch (e) {\n  throw new Error(\n    \"JSON parsing failed: \" + e.message +\n    \"\\nExtracted snippet:\\n\" + jsonText.slice(0, 300)\n  );\n}\nreturn [{\n  json: parsed\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -240,
        -2560
      ],
      "id": "f73ae8eb-61b9-4eb4-a111-b748abc49049",
      "name": "JSON Split Tasks1",
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "return $('JSON Split Tasks1').first().json.steps.map((s, i) => ({\n  json: {\n    step: s,\n    step_index: i,\n    transitions: $('JSON Split Tasks1').first().json.transitions\n  }\n}));\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2672,
        -2160
      ],
      "id": "d832e09a-9a5a-485f-9322-6e3296f82fad",
      "name": "Explode Steps1"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -2480,
        -2160
      ],
      "id": "1019b4a8-30ab-448c-aedf-081827d74f8e",
      "name": "Loop Over Items1"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nfor (const item of items) {\n  const j = item.json;\n  const content = j.message?.content || \"\";\n  const start = content.indexOf(\"{\");\n  const end = content.lastIndexOf(\"}\");\n  if (start === -1 || end === -1 || end <= start) {\n    throw new Error(\"No JSON object found in LLM message.content\");\n  }\n  const jsonText = content.slice(start, end + 1);\n  JSON.parse(jsonText);\n  j.message.content = jsonText;\n  item.json = j;\n}\nreturn items;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2464,
        -2624
      ],
      "id": "f1435cd0-7cdf-4ca8-9117-025e7d239e46",
      "name": "LLM Clean4",
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nfor (const item of items) {\n  const j = item.json;\n  const content = j.message?.content || \"\";\n  const start = content.indexOf(\"{\");\n  const end = content.lastIndexOf(\"}\");\n  if (start === -1 || end === -1 || end <= start) {\n    throw new Error(\"No JSON object found in LLM message.content\");\n  }\n  const jsonText = content.slice(start, end + 1);\n  JSON.parse(jsonText);\n  j.message.content = jsonText;\n  item.json = j;\n}\nreturn items;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2096,
        -2512
      ],
      "id": "127c1026-cd36-426f-8e2c-cf009c35f4c2",
      "name": "LLM Clean5",
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nfor (const item of items) {\n  const j = item.json;\n  const content = j.message?.content || \"\";\n  const start = content.indexOf(\"{\");\n  const end = content.lastIndexOf(\"}\");\n  if (start === -1 || end === -1 || end <= start) {\n    throw new Error(\"No JSON object found in LLM message.content\");\n  }\n  const jsonText = content.slice(start, end + 1);\n  JSON.parse(jsonText);\n  j.message.content = jsonText;\n  item.json = j;\n}\nreturn items;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1376,
        -2528
      ],
      "id": "ef56579d-efe0-43ff-8df4-d0352a6cf471",
      "name": "LLM Clean6",
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nfor (const item of items) {\n  const j = item.json;\n  const content = j.message?.content || \"\";\n  const start = content.indexOf(\"{\");\n  const end = content.lastIndexOf(\"}\");\n  if (start === -1 || end === -1 || end <= start) {\n    throw new Error(\"No JSON object found in LLM message.content\");\n  }\n  const jsonText = content.slice(start, end + 1);\n  JSON.parse(jsonText);\n  j.message.content = jsonText;\n  item.json = j;\n}\nreturn items;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -480,
        -2544
      ],
      "id": "c5efc763-074c-4c6e-a2d6-ddf530a2ece2",
      "name": "LLM Clean7",
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "42389661-7d31-4381-b299-0d365797537d",
              "name": "clean_ontology",
              "value": "={{ $json.message.content }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -1904,
        -2528
      ],
      "id": "f596a8e7-68aa-4a1d-83f4-e9df5a6a13d0",
      "name": "Set True Ontology1"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "42389661-7d31-4381-b299-0d365797537d",
              "name": "clean_rules",
              "value": "={{ $json.message.content }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -1184,
        -2544
      ],
      "id": "2af919cd-30a4-4d38-a90d-5375cfad7167",
      "name": "Set True Rules1"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nconst results = [];\nfor (const item of items) {\n  const data = item.json;\n  const wf = Array.isArray(data) ? data[0] : data;\n  const steps = wf.steps || [];\n  const transitions = wf.transitions || [];\n  const stepIds = new Set(steps.map(s => s.id));\n  const incoming = {};\n  const outgoing = {};\n  for (const id of stepIds) {\n    incoming[id] = 0;\n    outgoing[id] = 0;\n  }\n  const edgesWithMissingSteps = [];\n  for (const t of transitions) {\n    const { source_step_id, target_step_id } = t;\n    if (!stepIds.has(source_step_id) || !stepIds.has(target_step_id)) {\n      edgesWithMissingSteps.push(t);\n    }\n    if (stepIds.has(source_step_id)) {\n      outgoing[source_step_id] = (outgoing[source_step_id] || 0) + 1;\n    }\n    if (stepIds.has(target_step_id)) {\n      incoming[target_step_id] = (incoming[target_step_id] || 0) + 1;\n    }\n  }\n  const startSteps = [...stepIds].filter(id => (incoming[id] || 0) === 0);\n  const endSteps = [...stepIds].filter(id => (outgoing[id] || 0) === 0);\n  const visited = new Set();\n  const queue = [...startSteps];\n  while (queue.length > 0) {\n    const current = queue.shift();\n    if (visited.has(current)) continue;\n    visited.add(current);\n    for (const t of transitions) {\n      if (t.source_step_id === current && stepIds.has(t.target_step_id)) {\n        if (!visited.has(t.target_step_id)) {\n          queue.push(t.target_step_id);\n        }\n      }\n    }\n  }\n  const unreachableSteps = [...stepIds].filter(id => !visited.has(id));\n  const indegree = {};\n  for (const id of stepIds) indegree[id] = 0;\n  for (const t of transitions) {\n    if (stepIds.has(t.target_step_id)) {\n      indegree[t.target_step_id] = (indegree[t.target_step_id] || 0) + 1;\n    }\n  }\n  const zeroIn = [];\n  for (const id of stepIds) {\n    if ((indegree[id] || 0) === 0) zeroIn.push(id);\n  }\n  let processedCount = 0;\n  const indegreeCopy = { ...indegree };\n  const q2 = [...zeroIn];\n  while (q2.length > 0) {\n    const nodeId = q2.shift();\n    processedCount++;\n    for (const t of transitions) {\n      if (t.source_step_id === nodeId && stepIds.has(t.target_step_id)) {\n        indegreeCopy[t.target_step_id]--;\n        if (indegreeCopy[t.target_step_id] === 0) {\n          q2.push(t.target_step_id);\n        }\n      }\n    }\n  }\n  const hasCycle = processedCount < stepIds.size;\n  const outcomeIssues = [];\n  const stepById = {};\n  for (const s of steps) stepById[s.id] = s;\n  for (const t of transitions) {\n    if (!t.outcome_name) continue;\n    const srcStep = stepById[t.source_step_id];\n    if (!srcStep) continue;\n    const poss = srcStep.possible_outcomes || [];\n    const match = poss.some(o => o && o.name === t.outcome_name);\n    if (!match) {\n      outcomeIssues.push({\n        type: \"transition_outcome_not_defined_on_source_step\",\n        source_step_id: t.source_step_id,\n        target_step_id: t.target_step_id,\n        outcome_name: t.outcome_name,\n        description: `Transition outcome_name '${t.outcome_name}' is not listed in possible_outcomes of step '${t.source_step_id}'.`\n      });\n    }\n  }\n  for (const s of steps) {\n    const poss = s.possible_outcomes || [];\n    const outgoingForStep = transitions.filter(t => t.source_step_id === s.id);\n    const isEnd = outgoingForStep.length === 0;\n    for (const o of poss) {\n      if (!o || !o.name) continue;\n      const usedByTransition = outgoingForStep.some(t => t.outcome_name === o.name);\n      if (o.is_terminal && !usedByTransition && !isEnd) {\n        outcomeIssues.push({\n          type: \"terminal_outcome_not_routed\",\n          step_id: s.id,\n          outcome_name: o.name,\n          description: `Terminal outcome '${o.name}' on step '${s.id}' has no matching transition and the step still has outgoing edges.`\n        });\n      }\n    }\n  }\n  const endStepsMissingTerminal = [];\n  for (const endId of endSteps) {\n    const step = stepById[endId];\n    if (!step) continue;\n    const outcomes = step.possible_outcomes || [];\n    const hasTerminal = outcomes.some(o => o && o.is_terminal === true);\n    if (!hasTerminal) {\n      endStepsMissingTerminal.push({\n        step_id: endId,\n        label: step.label,\n        reason: \"End step has no possible_outcome marked is_terminal=true\"\n      });\n    }\n  }\n  const messages = [];\n  if (startSteps.length === 0) {\n    messages.push(\"No start steps detected (no steps without incoming transitions).\");\n  } else {\n    messages.push(`Start steps: ${startSteps.join(\", \")}`);\n  }\n  if (endSteps.length === 0) {\n    messages.push(\"No end steps detected (no steps without outgoing transitions).\");\n  } else {\n    messages.push(`End steps: ${endSteps.join(\", \")}`);\n  }\n  if (edgesWithMissingSteps.length > 0) {\n    messages.push(`Some transitions reference unknown steps (${edgesWithMissingSteps.length}).`);\n  }\n  if (unreachableSteps.length > 0) {\n    messages.push(`Unreachable steps from any start: ${unreachableSteps.join(\", \")}`);\n  }\n  if (endStepsMissingTerminal.length > 0) {\n    messages.push(\n      `End steps without terminal outcomes: ${endStepsMissingTerminal\n        .map(e => e.step_id)\n        .join(\", \")}`\n    );\n  }\n  if (outcomeIssues.length > 0) {\n    messages.push(`Outcome/transition inconsistencies detected (${outcomeIssues.length}).`);\n  }\n  if (hasCycle) {\n    messages.push(\"Cycle detected in the step graph (not a pure DAG).\");\n  }\n  if (\n    startSteps.length === 1 &&\n    endSteps.length >= 1 &&\n    edgesWithMissingSteps.length === 0 &&\n    unreachableSteps.length === 0 &&\n    endStepsMissingTerminal.length === 0 &&\n    outcomeIssues.length === 0 &&\n    !hasCycle\n  ) {\n    messages.push(\"Workflow looks structurally sane and traversable.\");\n  }\n  const sanityReport = {\n    is_traversable:\n      startSteps.length > 0 &&\n      edgesWithMissingSteps.length === 0 &&\n      unreachableSteps.length === 0,\n    has_cycle: hasCycle,\n    start_steps: startSteps,\n    end_steps: endSteps,\n    edges_with_missing_steps: edgesWithMissingSteps,\n    unreachable_steps: unreachableSteps,\n    end_steps_missing_terminal_outcome: endStepsMissingTerminal,\n    outcome_issues: outcomeIssues,\n    messages\n  };\n  results.push({ json: sanityReport });\n}\nreturn results;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        80,
        -2496
      ],
      "id": "8f37fdb3-df16-48f4-ad36-5b02c74a06a6",
      "name": "Sanity Check Report1"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "b178d190-ce5b-4540-9ee0-81c217dfc71b",
              "name": "job_description",
              "value": "={{ $json.step.description }}",
              "type": "string"
            },
            {
              "id": "008d2af5-3ed2-43f8-b3a7-e582d6ff9f4f",
              "name": "job_id",
              "value": "={{ $json.step.id }}",
              "type": "string"
            },
            {
              "id": "4bb73e76-2368-47d3-ad0c-9b4419111336",
              "name": "job_label",
              "value": "={{ $json.step.label }}",
              "type": "string"
            },
            {
              "id": "c827f984-688c-435b-879c-16196dc5f83d",
              "name": "preconditions_text",
              "value": "={{ $json.step.preconditions_text }}",
              "type": "string"
            },
            {
              "id": "f19418dc-4f21-495d-9f04-1418d26081f9",
              "name": "ouctomes",
              "value": "={{ $json.step.possible_outcomes }}",
              "type": "array"
            },
            {
              "id": "f08784e1-1fa5-4975-a268-ac9598b95961",
              "name": "relations",
              "value": "={{ $json.transitions }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -2272,
        -2144
      ],
      "id": "afdc4713-4ae4-49ef-b9f3-29d1a1dd59fe",
      "name": "Edit Fields1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8011/task-update",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{\n{ \"task_id\": $json.job_id,\n\"description\": $json.job_description,\n\"label\": $json.job_label\n}\n}}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2064,
        -2144
      ],
      "id": "5053c406-66eb-4341-9ca1-ec1827dc5b74",
      "name": "Task Feedback1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8011/relations",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ {'links': $('Edit Fields1').item.json.relations} }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2272,
        -2288
      ],
      "id": "616a8a34-8dd5-43bf-a278-86475797d088",
      "name": "Return Relations1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8011/ontology-update",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{JSON.parse($json.clean_ontology )}}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1744,
        -2528
      ],
      "id": "46e307be-6e8c-4ecd-8363-f9fd9175c6af",
      "name": "Upload Ontology1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8011/rules-update",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{\n    JSON.parse($json.clean_rules)\n}}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -944,
        -2544
      ],
      "id": "60d6de47-8816-49aa-847e-7a76514a0517",
      "name": "Upload Rules1"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({\n  model: \"llama3\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are an ontology extraction engine for industrial workflows.\\n\" +\n        \"You will receive up to three documents, clearly separated:\\n\" +\n        \"[WORKFLOW_SPEC]  - narrative description of the workflow\\n\" +\n        \"[WPS]            - formal Procedure Specification (if provided)\\n\" +\n        \"[DATASHEET]      - object datasheet(s) (if provided)\\n\\n\" +\n        \"From these, identify domain objects, processes, properties, and events.\\n\" +\n        \"Treat WORKFLOW_SPEC as the primary narrative.\\n\" +\n        \"Use WPS and DATASHEET only to add or clarify objects and processes, \" +\n        \"not to invent unrelated structures.\\n\\n\" +\n        \"Output STRICT JSON with keys: objects, processes, properties, events.\\n\" +\n        \"Do not include any other keys or free text.\"\n    },\n    {\n      role: \"user\",\n      content:\n        \"[WORKFLOW_SPEC]\\n\" +\n        $node[\"Webhook1\"].json.body.spec_text + \"\\n\\n\" +\n\n        \"[WPS]\\n\" +\n        ($node[\"Webhook1\"].json.body.wps_text || \"NOT PROVIDED\") + \"\\n\\n\" +\n\n        \"[DATASHEET]\\n\" +\n        ($node[\"Webhook1\"].json.body.datasheet_text || \"NOT PROVIDED\")\n    }\n  ],\n  stream: false\n}) }}\n",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2736,
        -2624
      ],
      "id": "104dc798-4a9f-4764-b3f4-a77663d04325",
      "name": "Extract Objects1"
    },
    {
      "parameters": {
        "url": "http://localhost:8011/reference-ontology",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -2944,
        -2624
      ],
      "id": "91450f60-fdd4-4a77-bc00-a8ff724e5c35",
      "name": "Fetch Reference Ontology1"
    },
    {
      "parameters": {
        "url": "http://localhost:8011/skills",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        -1856,
        -2144
      ],
      "id": "581454a7-14ab-4175-be84-a65abdd7e3cf",
      "name": "Fetch Skills1"
    },
    {
      "parameters": {
        "jsCode": "// Node: SUB: Check if Decision Point\nconst subtask = $input.first().json.step;\nconst isDecisionPoint = subtask.is_decision_point === true;\n\nreturn [{\n  json: {\n    ...subtask,\n    is_decision_point: isDecisionPoint,\n    decision_criteria: subtask.decision_criteria || null,\n    pass_condition: subtask.pass_condition || null,\n    fail_action: subtask.fail_action || 'notify_manager'\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2224,
        -1600
      ],
      "id": "78727b56-07a7-4076-9e08-a874e4e3dc18",
      "name": "Decision Point Code"
    },
    {
      "parameters": {
        "content": "## LLM  + Cleaning nodes\nWe want to make sure the LLM puts out a valid JSON schema, potentially bracketed by usual LLM waffle.\nClean waffle, check it's a valid JSON format, give it a second/third/... go if it isn't. LLama3 is generally capable - needs only 2 rounds max.",
        "height": 480,
        "width": 400
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1616,
        -2768
      ],
      "typeVersion": 1,
      "id": "c2c86a68-7c50-4951-8e01-651fac8e9f56",
      "name": "Sticky Note2"
    }
  ],
  "pinData": {},
  "connections": {
    "Execute Subtasks Sub-workflow": {
      "main": [
        [
          {
            "node": "Loop Over Items1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB-WORKFLOW: Trigger": {
      "main": [
        [
          {
            "node": "SUB: Subskills Splitter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: Subskills Splitter": {
      "main": [
        [
          {
            "node": "SUB: JSON Split Subtasks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: JSON Split Subtasks": {
      "main": [
        [
          {
            "node": "SUB: Explode Subtasks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: Explode Subtasks": {
      "main": [
        [
          {
            "node": "SUB: Loop Over Subtasks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: Loop Over Subtasks": {
      "main": [
        [],
        [
          {
            "node": "Decision Point Code",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: Extract Tasks": {
      "main": [
        [
          {
            "node": "SUB: Subtask Feedback",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SUB: Subtask Feedback": {
      "main": [
        [
          {
            "node": "SUB: Loop Over Subtasks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook1": {
      "main": [
        [
          {
            "node": "Setup1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ontology Normalisation1": {
      "main": [
        [
          {
            "node": "LLM Clean5",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Rules1": {
      "main": [
        [
          {
            "node": "LLM Clean6",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Task Partition1": {
      "main": [
        [
          {
            "node": "LLM Clean7",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Setup1": {
      "main": [
        [
          {
            "node": "Fetch Reference Ontology1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "JSON Split Tasks1": {
      "main": [
        [
          {
            "node": "Sanity Check Report1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Task Partition1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Explode Steps1": {
      "main": [
        [
          {
            "node": "Loop Over Items1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Items1": {
      "main": [
        [
          {
            "node": "Return Relations1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Edit Fields1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Clean4": {
      "main": [
        [
          {
            "node": "Ontology Normalisation1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Extract Objects1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Clean5": {
      "main": [
        [
          {
            "node": "Set True Ontology1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "LLM Clean4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Clean6": {
      "main": [
        [
          {
            "node": "Set True Rules1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Extract Rules1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Clean7": {
      "main": [
        [
          {
            "node": "JSON Split Tasks1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Task Partition1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set True Ontology1": {
      "main": [
        [
          {
            "node": "Upload Ontology1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set True Rules1": {
      "main": [
        [
          {
            "node": "Upload Rules1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Sanity Check Report1": {
      "main": [
        [
          {
            "node": "Explode Steps1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields1": {
      "main": [
        [
          {
            "node": "Task Feedback1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Task Feedback1": {
      "main": [
        [
          {
            "node": "Fetch Skills1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upload Ontology1": {
      "main": [
        [
          {
            "node": "Extract Rules1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upload Rules1": {
      "main": [
        [
          {
            "node": "Task Partition1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Objects1": {
      "main": [
        [
          {
            "node": "LLM Clean4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Reference Ontology1": {
      "main": [
        [
          {
            "node": "Extract Objects1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Skills1": {
      "main": [
        [
          {
            "node": "Execute Subtasks Sub-workflow",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Decision Point Code": {
      "main": [
        [
          {
            "node": "SUB: Extract Tasks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "saveDataErrorExecution": "all",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": false,
    "errorWorkflow": "vlJlmNoBNXlsGGvp"
  },
  "versionId": "3a9f0e15-d269-4468-a18d-39b8144e2a4e",
  "meta": {
    "instanceId": "d0b08ddea45ec199cb9cdc5a6db0b4b6e526d25a1fe4388cf11c3840b75c3662"
  },
  "id": "vlJlmNoBNXlsGGvp",
  "tags": []
}